{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9247fad3-3f20-4fde-a8b3-1d567a7681bb",
   "metadata": {},
   "source": [
    "# Meeting Transcription Analysis\n",
    "\n",
    "This notebook uses generative AI (GenAI) in the form of Large Language Models (LLMs) to analyze zoom meeting transcriptions.\n",
    "\n",
    "Analysis includes:\n",
    "- Main topics of discussion\n",
    "- Jargon used\n",
    "- Action items\n",
    "\n",
    "This notebook uses the [OpenAI REST API](https://platform.openai.com/docs/api-reference/introduction) to interact with LLMs hosted in a [FastChat](https://github.com/lm-sys/FastChat) deployment.\n",
    "\n",
    "*Disclaimer*: While developing this notebook, I used LLMs as a pair programmer to get template code for specific functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439193af-94c6-47f8-baed-f603c5cadbc9",
   "metadata": {},
   "source": [
    "## Install necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26b6863c-dedd-48c9-a1f4-4c5f19aa4d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai==0.28.1\n",
      "  Obtaining dependency information for openai==0.28.1 from https://files.pythonhosted.org/packages/1e/9f/385c25502f437686e4aa715969e5eaf5c2cb5e5ffa7c5cdd52f3c6ae967a/openai-0.28.1-py3-none-any.whl.metadata\n",
      "  Using cached openai-0.28.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: requests>=2.20 in /opt/conda/lib/python3.11/site-packages (from openai==0.28.1) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from openai==0.28.1) (4.66.1)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.11/site-packages (from openai==0.28.1) (3.8.5)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.11/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.11/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.20->openai==0.28.1) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.20->openai==0.28.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.20->openai==0.28.1) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.20->openai==0.28.1) (2023.7.22)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->openai==0.28.1) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp->openai==0.28.1) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.11/site-packages (from aiohttp->openai==0.28.1) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->openai==0.28.1) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp->openai==0.28.1) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp->openai==0.28.1) (1.3.1)\n",
      "Using cached openai-0.28.1-py3-none-any.whl (76 kB)\n",
      "Installing collected packages: openai\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.0.0rc3\n",
      "    Uninstalling openai-1.0.0rc3:\n",
      "      Successfully uninstalled openai-1.0.0rc3\n",
      "Successfully installed openai-0.28.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai==0.28.1 nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cffc89-acd3-4fc6-b297-3c15ed007c13",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1063dc39-9edc-417c-b546-8f2bd21653d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import openai\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b332e11b-d545-4a9d-a916-5cf32fa18fec",
   "metadata": {},
   "source": [
    "## Set Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e7f8051-41f7-4249-8ac8-bdf1723fd889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://sdsu-rci-fastchat.nrp-nautilus.io/v1\n"
     ]
    }
   ],
   "source": [
    "with open('env.yaml', 'r') as f:\n",
    "    env = yaml.safe_load(f)\n",
    "\n",
    "print(env[\"fastchat\"][\"base_url\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b1b43e-900c-41c9-903c-ea18318afdd9",
   "metadata": {},
   "source": [
    "## Importance of Token/Context Limit\n",
    "It is important to make effective use of an LLM's token limit or context window to make sure they are processing meaningful data. The Zoom video text track (.vtt) file has a lot of extra characters that we should remove, like timestamps, that would not be useful for our purposes.\n",
    "\n",
    "With that said, I conducted some preliminary, small-scale testing with the timestamps included and the LLM's analysis didn't appear to be adversely affected.\n",
    "\n",
    "Regardless, when dealing with nearly one hour of spoken words, it is wise to optimize your input length. Prior to any pre-processing, a simple run of the linux command `wc -w` revealed my transcript file had 10,815 \"words.\" \n",
    "\n",
    "For context, LLMs tokenization of words resulsts in a ratio of roughly 1 token = 3/4 of a word. So, 100 tokens is roughly 75 words. Using that ratio, we could expect the 10,815 words to be roughly equal to 13,519 tokens. And that doesn't include any user or system prompts!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2557c7c6-c344-409e-8f54-13a7051d28ed",
   "metadata": {},
   "source": [
    "## Clean the Zoom .vtt File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1ffd349-a86e-40fd-926b-7c546be9e941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw transcript character count: 64680\n",
      "Raw transcript word count: 10815\n",
      "Raw transcript line count: 1783\n"
     ]
    }
   ],
   "source": [
    "transcript_filename = env[\"transcript_filename\"]\n",
    "\n",
    "transcript_raw = \"\"\n",
    "\n",
    "with open(transcript_filename, 'r') as f:\n",
    "    transcript_raw = f.read()\n",
    "\n",
    "# Calculate and print info about raw file\n",
    "rawCharCount = len(transcript_raw)\n",
    "rawWordCount = len(transcript_raw.split())\n",
    "rawLineCount = len(transcript_raw.split(\"\\n\"))\n",
    "\n",
    "print(f\"Raw transcript character count: {rawCharCount}\")\n",
    "print(f\"Raw transcript word count: {rawWordCount}\")\n",
    "print(f\"Raw transcript line count: {rawLineCount}\")\n",
    "\n",
    "# Process transcript as a list to make it iterable\n",
    "transcript_transform = transcript_raw.split(\"\\n\")\n",
    "\n",
    "# Remove first two lines because they have no value\n",
    "transcript_transform = transcript_transform[2:]\n",
    "\n",
    "# Matches both numbered lines and timestamp lines\n",
    "digit_pattern = r\"^[0-9]+\"\n",
    "\n",
    "# Matches lines that start with a name\n",
    "name_pattern = r\"^[A-Z][a-z]+\\s[A-Z][a-z]+:\\s\"\n",
    "\n",
    "transcript_processed = []\n",
    "\n",
    "for line in transcript_transform:\n",
    "    # Ignore empty lines\n",
    "    if line == \"\":\n",
    "        continue\n",
    "    # Ignore timestamp and numbered lines\n",
    "    elif re.search(digit_pattern, line):\n",
    "        continue\n",
    "    # Strip off the names from lines that start with one\n",
    "    elif re.search(name_pattern, line):\n",
    "        substring_start_index = re.search(name_pattern, line).span()[1]\n",
    "        line = line[substring_start_index:]\n",
    "    transcript_processed.append(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff75789-9dbd-47af-aef9-56e17ac3df12",
   "metadata": {},
   "source": [
    "## Calculate and print info about processed file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ece7b852-66a1-4c7c-b6a9-16539abba06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8150\n"
     ]
    }
   ],
   "source": [
    "word_count = 0\n",
    "for line in transcript_processed:\n",
    "    word_count += len(line.split())\n",
    "    \n",
    "print(word_count)\n",
    "\n",
    "# Took the first 50 lines, but could optomize this based on the token length of the model\n",
    "# In this case I use Vicuna 33B v1.3 with 2048 max context length.\n",
    "transcript = ''.join(transcript_processed[:50])\n",
    "\n",
    "# # Maybe list comprehensions for the first two?\n",
    "# processedCharCount = \n",
    "# processedWordCount = \n",
    "# processedLineCount = len(transcript_processed)\n",
    "\n",
    "# print(f\"Processed transcript character count: {processedCharCount}\")\n",
    "# print(f\"Processed transcript word count: {processedWordCount}\")\n",
    "# print(f\"Processed transcript line count: {processedLineCount}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58310447-3211-43a2-a285-90b033d5bba6",
   "metadata": {},
   "source": [
    "## Configure OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47a88e00-fca1-4375-9175-31df324886e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"object\": \"list\",\n",
      "  \"data\": [\n",
      "    {\n",
      "      \"id\": \"vicuna-13b-v1.5-16k\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1699385409,\n",
      "      \"owned_by\": \"fastchat\",\n",
      "      \"root\": \"vicuna-13b-v1.5-16k\",\n",
      "      \"parent\": null,\n",
      "      \"permission\": [\n",
      "        {\n",
      "          \"id\": \"modelperm-3vLezhbzaxihC7kBAYtLwr\",\n",
      "          \"object\": \"model_permission\",\n",
      "          \"created\": 1699385409,\n",
      "          \"allow_create_engine\": false,\n",
      "          \"allow_sampling\": true,\n",
      "          \"allow_logprobs\": true,\n",
      "          \"allow_search_indices\": true,\n",
      "          \"allow_view\": true,\n",
      "          \"allow_fine_tuning\": false,\n",
      "          \"organization\": \"*\",\n",
      "          \"group\": null,\n",
      "          \"is_blocking\": false\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"vicuna-33b-v1.3\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1699385409,\n",
      "      \"owned_by\": \"fastchat\",\n",
      "      \"root\": \"vicuna-33b-v1.3\",\n",
      "      \"parent\": null,\n",
      "      \"permission\": [\n",
      "        {\n",
      "          \"id\": \"modelperm-huBKWewFFEVHQ7Vamm46hA\",\n",
      "          \"object\": \"model_permission\",\n",
      "          \"created\": 1699385409,\n",
      "          \"allow_create_engine\": false,\n",
      "          \"allow_sampling\": true,\n",
      "          \"allow_logprobs\": true,\n",
      "          \"allow_search_indices\": true,\n",
      "          \"allow_view\": true,\n",
      "          \"allow_fine_tuning\": false,\n",
      "          \"organization\": \"*\",\n",
      "          \"group\": null,\n",
      "          \"is_blocking\": false\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "openai.api_key = env[\"fastchat\"][\"api_key\"]\n",
    "openai.api_base = env[\"fastchat\"][\"base_url\"]\n",
    "\n",
    "# Test config by printing available models\n",
    "models = openai.Model.list()\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85144c1a-72b3-4305-9ad9-1fd63a9aebf4",
   "metadata": {},
   "source": [
    "## Ask the LLM to Perform the Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d378b53-447a-4cdc-9d16-7b210934b755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 3 items discussed were:\n",
      "\n",
      "* Vern is an acronym for Visionary Education, Research, Network Environment and is a Kubernetes-based instructional cluster that provides CPU and GPU nodes for students in machine learning, data science, big data, analytics, and other compute-intensive courses.\n",
      "* Vern is part of the National Research Platform (NRP) and Jupiter Hub is the front door for accessing the cluster.\n",
      "* Kubernetes is an open-source software from Google that manages containers and is used to manage the Vern cluster.\n",
      "\n",
      "A short 2 or 3 sentence summary:\n",
      "Vern is an instructional cluster that provides CPU and GPU nodes to students for machine learning, data science, big data, analytics, and other compute-intensive courses. It is part of the National Research Platform and Jupiter Hub is the front door for accessing it. Kubernetes is used to manage the cluster, which is container-based and provides a software factory for creating containers around software.\n",
      "\n",
      "3 action items:\n",
      "\n",
      "1. Contact Kyle Crick to get access to Vern and Jupiter Hub.\n",
      "2. Look into Argus software and see if it can be added to Vern.\n",
      "3. Brief faculty members from accounting, finance, and statistics on the concept of Kubernetes.\n"
     ]
    }
   ],
   "source": [
    "# Model can be replaced with the model id from the previous call\n",
    "model = \"vicuna-33b-v1.3\"\n",
    "prompt = transcript\n",
    "\n",
    "# create a chat completion\n",
    "completion = openai.ChatCompletion.create(\n",
    "  model=model,\n",
    "  messages=[\n",
    "      {\"role\": \"system\", \"content\": \"You will be given a meeting transcript. From this transcript: Provide the top 3 items discussed. Provide a short 2 or 3 sentence summary. Provide 3 action items.\"},\n",
    "      {\"role\": \"user\", \"content\": prompt}\n",
    "  ]\n",
    ")\n",
    "\n",
    "# print the completion\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434f786e-0179-413d-ba5c-33922e3e4df1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
