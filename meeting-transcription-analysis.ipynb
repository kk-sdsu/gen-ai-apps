{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9247fad3-3f20-4fde-a8b3-1d567a7681bb",
   "metadata": {},
   "source": [
    "# Meeting Transcription Analysis\n",
    "\n",
    "This notebook uses generative AI (GenAI) in the form of Large Language Models (LLMs) to analyze zoom meeting transcriptions.\n",
    "\n",
    "Analysis includes:\n",
    "- Main topics of discussion\n",
    "- Jargon used\n",
    "- Action items\n",
    "\n",
    "This notebook uses the [OpenAI REST API](https://platform.openai.com/docs/api-reference/introduction) to interact with LLMs hosted in a [FastChat](https://github.com/lm-sys/FastChat) deployment.\n",
    "\n",
    "*Disclaimer*: While developing this notebook, I used LLMs as a pair programmer to get template code for specific functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439193af-94c6-47f8-baed-f603c5cadbc9",
   "metadata": {},
   "source": [
    "## Install necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26b6863c-dedd-48c9-a1f4-4c5f19aa4d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai==0.28.1\n",
      "  Obtaining dependency information for openai==0.28.1 from https://files.pythonhosted.org/packages/1e/9f/385c25502f437686e4aa715969e5eaf5c2cb5e5ffa7c5cdd52f3c6ae967a/openai-0.28.1-py3-none-any.whl.metadata\n",
      "  Using cached openai-0.28.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting nltk\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: requests>=2.20 in /opt/conda/lib/python3.11/site-packages (from openai==0.28.1) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from openai==0.28.1) (4.66.1)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.11/site-packages (from openai==0.28.1) (3.8.5)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.11/site-packages (from nltk) (1.3.2)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Obtaining dependency information for regex>=2021.8.3 from https://files.pythonhosted.org/packages/8d/6b/2f6478814954c07c04ba60b78d688d3d7bab10d786e0b6c1db607e4f6673/regex-2023.12.25-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading regex-2023.12.25-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m742.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.20->openai==0.28.1) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.20->openai==0.28.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.20->openai==0.28.1) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.20->openai==0.28.1) (2023.7.22)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->openai==0.28.1) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp->openai==0.28.1) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.11/site-packages (from aiohttp->openai==0.28.1) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->openai==0.28.1) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp->openai==0.28.1) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp->openai==0.28.1) (1.3.1)\n",
      "Using cached openai-0.28.1-py3-none-any.whl (76 kB)\n",
      "Downloading regex-2023.12.25-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (785 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m785.1/785.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: regex, nltk, openai\n",
      "Successfully installed nltk-3.8.1 openai-0.28.1 regex-2023.12.25\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai==0.28.1 nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cffc89-acd3-4fc6-b297-3c15ed007c13",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1063dc39-9edc-417c-b546-8f2bd21653d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import openai\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b332e11b-d545-4a9d-a916-5cf32fa18fec",
   "metadata": {},
   "source": [
    "## Set Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e7f8051-41f7-4249-8ac8-bdf1723fd889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://sdsu-rci-fastchat.nrp-nautilus.io/v1\n"
     ]
    }
   ],
   "source": [
    "with open('env.yaml', 'r') as f:\n",
    "    env = yaml.safe_load(f)\n",
    "\n",
    "print(env[\"fastchat\"][\"base_url\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b1b43e-900c-41c9-903c-ea18318afdd9",
   "metadata": {},
   "source": [
    "## Importance of Token/Context Limit\n",
    "It is important to make effective use of an LLM's token limit or context window to make sure they are processing meaningful data. The Zoom video text track (.vtt) file has a lot of extra characters that we should remove, like timestamps, that would not be useful for our purposes.\n",
    "\n",
    "With that said, I conducted some preliminary, small-scale testing with the timestamps included and the LLM's analysis didn't appear to be adversely affected.\n",
    "\n",
    "Regardless, when dealing with nearly one hour of spoken words, it is wise to optimize your input length. Prior to any pre-processing, a simple run of the linux command `wc -w` revealed my transcript file had 10,815 \"words.\" \n",
    "\n",
    "For context, LLMs tokenization of words resulsts in a ratio of roughly 1 token = 3/4 of a word. So, 100 tokens is roughly 75 words. Using that ratio, we could expect the 10,815 words to be roughly equal to 13,519 tokens. And that doesn't include any user or system prompts!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2557c7c6-c344-409e-8f54-13a7051d28ed",
   "metadata": {},
   "source": [
    "## Clean the Zoom .vtt File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1ffd349-a86e-40fd-926b-7c546be9e941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw transcript character count: 64680\n",
      "Raw transcript word count: 10815\n",
      "Raw transcript line count: 1783\n"
     ]
    }
   ],
   "source": [
    "transcript_filename = env[\"transcript_filename\"]\n",
    "\n",
    "transcript_raw = \"\"\n",
    "\n",
    "with open(transcript_filename, 'r') as f:\n",
    "    transcript_raw = f.read()\n",
    "\n",
    "# Calculate and print info about raw file\n",
    "rawCharCount = len(transcript_raw)\n",
    "rawWordCount = len(transcript_raw.split())\n",
    "rawLineCount = len(transcript_raw.split(\"\\n\"))\n",
    "\n",
    "print(f\"Raw transcript character count: {rawCharCount}\")\n",
    "print(f\"Raw transcript word count: {rawWordCount}\")\n",
    "print(f\"Raw transcript line count: {rawLineCount}\")\n",
    "\n",
    "# Process transcript as a list to make it iterable\n",
    "transcript_transform = transcript_raw.split(\"\\n\")\n",
    "\n",
    "# Remove first two lines because they have no value\n",
    "transcript_transform = transcript_transform[2:]\n",
    "\n",
    "# Matches both numbered lines and timestamp lines\n",
    "digit_pattern = r\"^[0-9]+\"\n",
    "\n",
    "# Matches lines that start with a name\n",
    "name_pattern = r\"^[A-Z][a-z]+\\s[A-Z][a-z]+:\\s\"\n",
    "\n",
    "transcript_processed = []\n",
    "\n",
    "for line in transcript_transform:\n",
    "    # Ignore empty lines\n",
    "    if line == \"\":\n",
    "        continue\n",
    "    # Ignore timestamp and numbered lines\n",
    "    elif re.search(digit_pattern, line):\n",
    "        continue\n",
    "    # Strip off the names from lines that start with one\n",
    "    elif re.search(name_pattern, line):\n",
    "        substring_start_index = re.search(name_pattern, line).span()[1]\n",
    "        line = line[substring_start_index:]\n",
    "    transcript_processed.append(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff75789-9dbd-47af-aef9-56e17ac3df12",
   "metadata": {},
   "source": [
    "## Calculate and print info about processed file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ece7b852-66a1-4c7c-b6a9-16539abba06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8150\n"
     ]
    }
   ],
   "source": [
    "word_count = 0\n",
    "for line in transcript_processed:\n",
    "    word_count += len(line.split())\n",
    "    \n",
    "print(word_count)\n",
    "\n",
    "# Took the first 50 lines, but could optomize this based on the token length of the model\n",
    "# In this case I use Vicuna 33B v1.3 with 2048 max context length.\n",
    "transcript = ''.join(transcript_processed[:50])\n",
    "\n",
    "# # Maybe list comprehensions for the first two?\n",
    "# processedCharCount = \n",
    "# processedWordCount = \n",
    "# processedLineCount = len(transcript_processed)\n",
    "\n",
    "# print(f\"Processed transcript character count: {processedCharCount}\")\n",
    "# print(f\"Processed transcript word count: {processedWordCount}\")\n",
    "# print(f\"Processed transcript line count: {processedLineCount}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58310447-3211-43a2-a285-90b033d5bba6",
   "metadata": {},
   "source": [
    "## Configure OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47a88e00-fca1-4375-9175-31df324886e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"object\": \"list\",\n",
      "  \"data\": [\n",
      "    {\n",
      "      \"id\": \"vicuna-13b-v1.5-16k\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1706900577,\n",
      "      \"owned_by\": \"fastchat\",\n",
      "      \"root\": \"vicuna-13b-v1.5-16k\",\n",
      "      \"parent\": null,\n",
      "      \"permission\": [\n",
      "        {\n",
      "          \"id\": \"modelperm-97JyKKoTFjGKFvJSnzwJjT\",\n",
      "          \"object\": \"model_permission\",\n",
      "          \"created\": 1706900577,\n",
      "          \"allow_create_engine\": false,\n",
      "          \"allow_sampling\": true,\n",
      "          \"allow_logprobs\": true,\n",
      "          \"allow_search_indices\": true,\n",
      "          \"allow_view\": true,\n",
      "          \"allow_fine_tuning\": false,\n",
      "          \"organization\": \"*\",\n",
      "          \"group\": null,\n",
      "          \"is_blocking\": false\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"vicuna-33b-v1.3\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1706900577,\n",
      "      \"owned_by\": \"fastchat\",\n",
      "      \"root\": \"vicuna-33b-v1.3\",\n",
      "      \"parent\": null,\n",
      "      \"permission\": [\n",
      "        {\n",
      "          \"id\": \"modelperm-3RbB5LgGNxUBxiXSeKfq6X\",\n",
      "          \"object\": \"model_permission\",\n",
      "          \"created\": 1706900577,\n",
      "          \"allow_create_engine\": false,\n",
      "          \"allow_sampling\": true,\n",
      "          \"allow_logprobs\": true,\n",
      "          \"allow_search_indices\": true,\n",
      "          \"allow_view\": true,\n",
      "          \"allow_fine_tuning\": false,\n",
      "          \"organization\": \"*\",\n",
      "          \"group\": null,\n",
      "          \"is_blocking\": false\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "openai.api_key = env[\"fastchat\"][\"api_key\"]\n",
    "openai.api_base = env[\"fastchat\"][\"base_url\"]\n",
    "\n",
    "# Test config by printing available models\n",
    "models = openai.Model.list()\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85144c1a-72b3-4305-9ad9-1fd63a9aebf4",
   "metadata": {},
   "source": [
    "## Ask the LLM to Perform the Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d378b53-447a-4cdc-9d16-7b210934b755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 3 items discussed are:\n",
      "\n",
      "1. Vern is an instructional cluster that provides CPU and GPU resources to students in machine learning, data science, big data, and analytics courses.\n",
      "2. Vern is part of a nationally distributed Kubernetes cluster, which is managed by the National Research Platform team and provides technical support and priority scheduling.\n",
      "3. Vern is open for use by the National Research community, and any spare capacity is available to researchers.\n",
      "\n",
      "A short 2 or 3 sentence summary is that Vern is an instructional cluster that provides resources for students in data science, machine learning, and big data courses. It is part of a nationally distributed Kubernetes cluster and is managed by the National Research Platform team.\n",
      "\n",
      "Three action items are:\n",
      "\n",
      "1. Researchers should familiarize themselves with Vern and Jupiter Hub.\n",
      "2. Researchers should work with the Kubernetes software factory to containerize their software and run it on Vern.\n",
      "3. Finance department members should work with the IT department to determine if they can use Jupiter Hub and the software factory to run Argus on Vern.\n"
     ]
    }
   ],
   "source": [
    "# Model can be replaced with the model id from the previous call\n",
    "model = \"vicuna-33b-v1.3\"\n",
    "prompt = transcript\n",
    "\n",
    "# create a chat completion\n",
    "completion = openai.ChatCompletion.create(\n",
    "  model=model,\n",
    "  messages=[\n",
    "      {\"role\": \"system\", \"content\": \"You will be given a meeting transcript. From this transcript: Provide the top 3 items discussed. Provide a short 2 or 3 sentence summary. Provide 3 action items.\"},\n",
    "      {\"role\": \"user\", \"content\": prompt}\n",
    "  ]\n",
    ")\n",
    "\n",
    "# print the completion\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434f786e-0179-413d-ba5c-33922e3e4df1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
